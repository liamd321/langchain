{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8e0cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "#pip install langchain langchain-openai langchain-community\n",
    "#pip install langchain langchain-openai langchain-community"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d233d60",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "# LangChain Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771e1788",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "\n",
    "## Motivation\n",
    "\n",
    "##### Why LangChain?\n",
    "\n",
    "LangChain is not essential for the creation of agents, other tools exist such the software development kit (SDK) for different models, however there are some significant advantages to using LangChain that should be highlighted\n",
    "\n",
    "\n",
    "1. Abstraction and Simplification\n",
    "    \n",
    "    - LangChain provides a high-level abstraction over the complexities of working with LLMs. It simplifies the process of integrating LLMs into applications by providing pre-built components and workflows, allowing developers to focus on building their applications rather than dealing with low-level details.\n",
    "\n",
    "\n",
    "2. Modular and Composable \n",
    "    \n",
    "    - LangChain is designed to be modular and composable, allowing developers to easily combine different components and functionalities. This modularity enables the creation of complex applications by piecing simple blocks together.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7e7a32",
   "metadata": {},
   "source": [
    "Through this document, there will be a few marked tasks that will partially rely on your knowledge of python [dictionaries](https://docs.python.org/3/tutorial/datastructures.html#dictionaries)\n",
    "\n",
    "[Task 1](#task1)\n",
    "[Task 2](#task2)\n",
    "[Task 3](#task3)\n",
    "[Task 4](#task4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4786f6a8",
   "metadata": {},
   "source": [
    "To get started, define your generated key here from https://platform.openai.com/api-keys: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54d5643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<???>\" #add your key here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50acdb5",
   "metadata": {},
   "source": [
    "The next few blocks are a quick intro to the basic syntax and use of langchain, specifically using openai's 4o-mini model (free)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1a7bd5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The capital of France is Paris.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 14, 'total_tokens': 21, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CR7ORjHUqx0dJIUnJ32uOT7BdZ08d', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--783c4ff4-5cd0-4130-9901-0e2ae456bee1-0' usage_metadata={'input_tokens': 14, 'output_tokens': 7, 'total_tokens': 21, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_openai.llms import OpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "model = ChatOpenAI(model='gpt-4o-mini')\n",
    "prompt = [HumanMessage(\"What is the capital of France?\")]\n",
    "output = model.invoke(prompt)\n",
    "print(output)\n",
    "print(output.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69db378c",
   "metadata": {},
   "source": [
    "To break down the above, HumanMessage is input you would type into a chat model, and our outputted AIMessage is the response object, with the content of the message, plus some metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfef39f",
   "metadata": {},
   "source": [
    "Below we do the same, but using slightly different syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2e238813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langchain is a framework designed for building applications that leverage large language models (LLMs) by providing tools and components for integrating these models with various data sources, APIs, and other functionalities. It facilitates the development of complex workflows that combine natural language processing capabilities with external data retrieval, memory management, and user interactions. By offering abstractions for prompt management, chaining together multiple LLM calls, and connecting to different backends, Langchain enables developers to create sophisticated applications such as chatbots, question-answering systems, and more, while simplifying the process of managing the underlying components necessary to interact with language models effectively.\n"
     ]
    }
   ],
   "source": [
    "#chat query\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "response = llm.invoke(\"Explain langchain in one paragraph.\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6d585b",
   "metadata": {},
   "source": [
    "This invoke() method is a simple and straightforward way to get a response from a model, and it's naming should harken back to the fundamental idea of LLMs as token predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1912113",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "While we may not give a detailed breakdown, while going through this document make sure you can run and understand all these cells, and it could be useful to take note of the ouputs we are getting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe65f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Imagine your regular computer is like a really fast librarian who can find books for you in a huge library. This librarian can only look at one book at a time, but they can do it super quickly. \n",
      "\n",
      "Now, think of quantum computing like a magical librarian. This librarian can look at many books at the same time! This is because of something called \"quantum bits,\" or \"qubits.\" \n",
      "\n",
      "In a regular computer, the smallest piece of information is a \"bit,\" which can be either a 0 or a 1, like a light switch that can be off or on. But a qubit can be both 0 and 1 at the same time, kind of like a spinning coin that is both heads and tails while it's spinning. This special ability allows quantum computers to solve certain problems much faster than regular computers.\n",
      "\n",
      "So, when you have a really tough question, like finding the best route for a delivery truck or cracking a secret code, a quantum computer can zoom through all the possibilities way quicker than a regular computer. It's like having a super-powered librarian who can find the best books for you in no time!\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "template = ChatPromptTemplate.from_template(\"Explain {topic} like I'm {age} years old.\")\n",
    "prompt = template.format_messages(topic='quantum computing', age=12)\n",
    "response = llm.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30325ed6",
   "metadata": {},
   "source": [
    "Above, LangChain's ChatPromptTemplate allows us to define and then pass in arguments to a custom response template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2afe5f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "##  Roles in LangChain Conversations\n",
    "\n",
    "In LangChain, messages in a conversation are organized by roles, similar to how a lot of models work internally. Each role defines who is speaking and how that message should be interpreted.\n",
    "\n",
    "| Role     | Description | Example Use |\n",
    "|---------------|------------------|------------------|\n",
    "| `system` | Sets the overall behavior, tone, or context for the assistant. | `\"You are a helpful assistant that summarizes legal documents in plain English.\"` |\n",
    "| `user` | Represents the human’s input or request. | `\"Summarize this contract for me in 3 bullet points.\"` |\n",
    "| `assistant` | Represents the AI model’s reply or reasoning. | `\"Here’s a simplified summary of the contract...\"` |\n",
    "\n",
    "To see how these messages are actually implemented and passed to the LLM, check out [the langchain documentation](https://python.langchain.com/docs/concepts/messages/), but the key utility to be aware of is that langchain standardizes these messages for different model architectures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fdd67702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of France is Paris!!!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 33, 'total_tokens': 40, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_51db84afab', 'id': 'chatcmpl-CR7VGuwyTGQPihRE5DXiOfx9gd4fv', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--34831d4e-984a-400e-b998-eb34adc11195-0', usage_metadata={'input_tokens': 33, 'output_tokens': 7, 'total_tokens': 40, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "model = ChatOpenAI(model='gpt-4o-mini')\n",
    "system_msg = SystemMessage(\"You are a helpful assistant that responds to questions with three exclamation marks.\")\n",
    "human_msg=HumanMessage('What is the capital of France?')\n",
    "model.invoke([system_msg, human_msg])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383edc4f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "As we can see from the above examples, the prompt (system message + human message) significantly changes the output assistant message. In a prompt such as above, the context (in this case system_msg) and the question (human_msg) are hardcoded, but it is sometimes much more conveinient to give these to the model dynamically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafffb38",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "### Task 1 <a class=\"anchor\" id=\"task1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d68629c",
   "metadata": {},
   "source": [
    "Below, uncomment and use the syntax provided to create a dynamic prompt containing text, context and a question using the PromptTemplate module. We will pass this to a model later, so ask it a question you are interested in, along with some context you want it to draw from!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ed6d4407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"Explain quantum computing like I'm 12 years old.\", additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "template = PromptTemplate.from_template(\"\"\"Answer the question based on the context below. If the question cannot be answered using the information provided, answer with \"I don't know\". Context: {context} Question: {question} Answer:\"\"\")\n",
    "#prompt = template.invoke({\"context\":\"\"\"Your context here\"\"\",\"question\":\"\"\"Your question here\"\"\"})\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7dffc8",
   "metadata": {},
   "source": [
    "A quick note: langchain here lets you use braces {} to denote variables that you will pass in later, similar to python's [f-string syntax](https://docs.python.org/3/tutorial/inputoutput.html#the-string-format-method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9a0df869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Answer the question based on the context below. If the question cannot be answered using the information provided, answer with \"I don\\'t know\". Context: The most recent advancedements in NLP are being driven by LLMS. These models benefit greatly from large size and are used by devs working with Natural Language Processing. Developers can use these models through Hugging Face\\'s \\'transformers\\' library, or by utilizing OPenAI and Cohere\\'s offering through the \\'openai\\'\\n                           and \\'cohere\\' libraries, respectively. Question: Which model providers provide LLMs? Answer:')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example answer\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "template = PromptTemplate.from_template(\"\"\"Answer the question based on the context below. If the question cannot be answered using the information provided, answer with \"I don't know\". Context: {context} Question: {question} Answer:\"\"\")\n",
    "prompt = template.invoke({\"context\":\"\"\"The most recent advancedements in NLP are being driven by LLMS. These models benefit greatly from large size and are used by devs working with Natural Language Processing. Developers can use these models through Hugging Face's 'transformers' library, or by utilizing OPenAI and Cohere's offering through the 'openai'\n",
    "                           and 'cohere' libraries, respectively.\"\"\", \n",
    "                 \"question\":\"Which model providers provide LLMs?\"})\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fefee3",
   "metadata": {},
   "source": [
    "Now we pass it to the model like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "885b13cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' OpenAI and Cohere.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = OpenAI(model='gpt-4o-mini')\n",
    "completion = model.invoke(prompt)\n",
    "completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2b8bcf",
   "metadata": {},
   "source": [
    "Hopefully you can now see that we can now pass models these prompts with context without needing to break them up. For fun, go back and change your question to one the model shouldn't be able to answer based on the context to see the result!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a7e0bb",
   "metadata": {},
   "source": [
    "Here's another syntax that can be used for building AI chat applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "944be97f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=\"Answer the qestuon based on the context below. If the question cannot be answered using the information provided, answer with 'I don't know'.\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='Question: {question}.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='Context: {context}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "template = ChatPromptTemplate.from_messages([('system',\"Answer the qestuon based on the context below. If the question cannot be answered using the information provided, answer with 'I don't know'.\"),\n",
    "                                             ('human','Question: {question}.'), ('human','Context: {context}')])\n",
    "template.invoke({\"context\":\"\"\"The most recent advancedements in NLP are being driven by LLMS. These models benefit greatly from large size and are used by devs working with Natural Language Processing. Developers can use these models through Hugging Face's 'transformers' library, or by utilizing OPenAI and Cohere's offering through the 'openai'\n",
    "                           and 'cohere' libraries, respectively.\"\"\",\n",
    "                 \"question\":\"Which model providers provide LLMs?\"})\n",
    "template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcf5841",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "\n",
    "There's a (small) chance you may be thinking: \n",
    "\n",
    "\n",
    "\"Okay, so — this is all well and good...  \n",
    "BUT!!! These plain text responses??? ARE. YOU. KIDDING. ME?!   \n",
    "\n",
    "I mean, they're fine... but like… WHERE’S THE FORMATTING MAGIC?!   \n",
    "  \n",
    "How do I get specific formats out of LLMs for my use case?!?!\" \n",
    "\n",
    "\n",
    "Wow I am so glad you asked.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd90e8a1",
   "metadata": {},
   "source": [
    "Others you may have heard of can be useful like CSV (tabular data, sheets) and XML, but the most common format to generate is JSON. Here's a quick example of how to get a specific format output from a model:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "172b608b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoghurttub/LLM_FYP/.venv/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1915: UserWarning: Received a Pydantic BaseModel V1 schema. This is not supported by method=\"json_schema\". Please use method=\"function_calling\" or specify schema via JSON Schema or Pydantic V2 BaseModel. Overriding to method=\"function_calling\".\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AnswerWithJustification(answer='A pound of bricks and a pound of feathers weigh the same.', justification='Both are measured as one pound, so regardless of the material, a pound is a pound.')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "class AnswerWithJustification(BaseModel):\n",
    "    \"An answer to the user's question with justification for the answer\"\n",
    "    answer: str\n",
    "    '''The answer to the user's question'''\n",
    "    justification: str\n",
    "    '''The justification for the answer'''\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "structured_llm = llm.with_structured_output(AnswerWithJustification)\n",
    "\n",
    "structured_llm.invoke(\"\"\"What weighs more, a pounnd of bricks or a pound of feathers?\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d811aca",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "### So, what's going on here?\n",
    "\n",
    "The first thing that's happening is the definition of a format or 'schema' that you want the LLM to respect when producing the output.\n",
    "\n",
    "We have done this using Pydantic, and converted our schema to  a JSON object called a JSONSchema that describes data. This will be passed to the LLM, and langchain chooses how to do this for us.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfe74f2",
   "metadata": {},
   "source": [
    "Then, you include that format in the prompt, along with the text you want to use as the source.\n",
    "\n",
    "Pydantic will also let us check that our data matches our schema.\n",
    "\n",
    "Now you try!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178f49c6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "### Task 2 <a class=\"anchor\" id=\"task2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed4df81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "class AnswerWithJustification(BaseModel):\n",
    "    #copy the syntax above and create a template/'schema' for your desired output\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "structured_llm = llm.with_structured_output(AnswerWithJustification)\n",
    "\n",
    "structured_llm.invoke(\"\"\"ask your question here\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c083aa",
   "metadata": {},
   "source": [
    "If you really are super interested in other formats, there are things called output parsers that can structure the responses that you get from  a LLM, provided by langchain.\n",
    "\n",
    "These parsers provide the format instructions and validate the output, much like Pydantic above.\n",
    "\n",
    "Here's a quick example using a parser that seperates a string into a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "872e4181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'banana', 'cherry']\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "parser = CommaSeparatedListOutputParser()\n",
    "items = parser.invoke(\"apple, banana, cherry\")\n",
    "print(items)\n",
    "print(items.__class__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39076244",
   "metadata": {},
   "source": [
    "These parsers are going to be super useful when we are (soon) assembling all these little pieces of knowledge into an LLM application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e773571e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "So far we have been turning single inputs into single outputs (using the invoke() method). However, in real world applications, we often need to handle multiple inputs and outputs. This is where **chains** come into play.\n",
    "\n",
    "Expanding from invoke(), we can use batch() to transform a batch of inputs to outputs and .stream to 'stream' outputs from a single input as it's produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "66667b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the scarecrow win an award? \n",
      "\n",
      "Because he was outstanding in his field!\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "model = ChatOpenAI(model = 'gpt-4o-mini', temperature=0)\n",
    "completion = model.invoke(\"Tell me a joke.\")\n",
    "print(completion.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0d269f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "### Task 3 <a class=\"anchor\" id=\"task3\"></a>\n",
    "\n",
    "Have a go at the batch method, by prompting the model with two or more questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043762bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "completions = model.batch([???# prompt the model, ??? prompt the model again])\n",
    "for i, completion in enumerate(completions):\n",
    "    message = completion.content\n",
    "    print(f\"Response {i+1}: {message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "201fc7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 1: Why did the scarecrow win an award? \n",
      "\n",
      "Because he was outstanding in his field!\n",
      "Response 2: Did you know that honey never spoils? Archaeologists have found pots of honey in ancient Egyptian tombs that are over 3,000 years old and still perfectly edible! Honey's low moisture content and acidic pH create an inhospitable environment for bacteria and microorganisms, allowing it to last indefinitely.\n"
     ]
    }
   ],
   "source": [
    "#Exmaple answer\n",
    "completions = model.batch([\"Tell me a joke.\", \"Tell me a fun fact\"])\n",
    "for i, completion in enumerate(completions):\n",
    "    message = completion.content\n",
    "    print(f\"Response {i+1}: {message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e716f583",
   "metadata": {},
   "source": [
    "Here we break up the output by token using the stream() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f0a79a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Good\n",
      "bye\n",
      "!\n",
      " If\n",
      " you\n",
      " have\n",
      " any\n",
      " more\n",
      " questions\n",
      " in\n",
      " the\n",
      " future\n",
      ",\n",
      " feel\n",
      " free\n",
      " to\n",
      " ask\n",
      ".\n",
      " Take\n",
      " care\n",
      "!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for token in model.stream('Bye!'):\n",
    "    print(token.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34b8d57",
   "metadata": {},
   "source": [
    "It might not be so clear what stream() is doing. It takes one input, and as new tokens/outputs become available from the model, it outputs them. Sometimes this streaming isn't supported, and langchain will output a single part containing all the ouputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c944db",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "\n",
    "### So, what now?\n",
    "\n",
    "\n",
    "So far we have been doing what are called 'imperative' calls or 'composition'.\n",
    "\n",
    "This means we have been directly calling.\n",
    "\n",
    "Moving forward, for an expanded toolkit, we will adopt a 'declarative' approach on top of this.\n",
    "\n",
    "We will take advantage of a feature of LangChain called LangChain Expression Language or LCEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db59691",
   "metadata": {},
   "source": [
    "#### To summarise what we have done so far,\n",
    "\n",
    "let's *imperatively compose* a simple example of a translation chatbot using techniques we have (mostly) already seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ae1f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"J'aime la programmation.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([('system',\"You are a helpful assistant that translates {input_language} to {output_language}.\"),('human',\"{text_to_translate}\")])\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "@chain\n",
    "def chatbot(values):\n",
    "    prompt = template.invoke(values)\n",
    "    return model.invoke(prompt).content\n",
    "#use it\n",
    "chatbot.invoke({\"input_language\":\"English\", \"output_language\":\"French\", \"text_to_translate\":\"I love programming.\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59a4784",
   "metadata": {},
   "source": [
    "The above is an example of this single input single output iterative stuff we are talking about, and below is an example of the iterative approach to streaming our outputs, which we will then automate with LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1986a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "J\n",
      "'aime\n",
      " la\n",
      " programmation\n",
      ".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@chain\n",
    "def chatbot(values):\n",
    "    prompt = template.invoke(values)\n",
    "    for token in model.stream(prompt):\n",
    "        yield token.content\n",
    "for part in chatbot.stream({\"input_language\":\"English\", \"output_language\":\"French\", \"text_to_translate\":\"I love programming.\"}):\n",
    "    print(part)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f93144e",
   "metadata": {},
   "source": [
    "### LCEL Example!:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcde2b6f",
   "metadata": {},
   "source": [
    "LCEL allows for parallel execution, streaming and asynchronous execution automatically, seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "183b7151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={} response_metadata={} id='run--eaaefd8c-f0d6-4767-94c0-d4b21619ce5b'\n",
      "content='J' additional_kwargs={} response_metadata={} id='run--eaaefd8c-f0d6-4767-94c0-d4b21619ce5b'\n",
      "content=\"'aime\" additional_kwargs={} response_metadata={} id='run--eaaefd8c-f0d6-4767-94c0-d4b21619ce5b'\n",
      "content=' la' additional_kwargs={} response_metadata={} id='run--eaaefd8c-f0d6-4767-94c0-d4b21619ce5b'\n",
      "content=' programmation' additional_kwargs={} response_metadata={} id='run--eaaefd8c-f0d6-4767-94c0-d4b21619ce5b'\n",
      "content='.' additional_kwargs={} response_metadata={} id='run--eaaefd8c-f0d6-4767-94c0-d4b21619ce5b'\n",
      "content='' additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'service_tier': 'default'} id='run--eaaefd8c-f0d6-4767-94c0-d4b21619ce5b'\n"
     ]
    }
   ],
   "source": [
    "chatbot = template | model\n",
    "# this is using the template and model we defined above\n",
    "# template = ChatPromptTemplate.from_messages([('system',\"You are a helpful assistant that translates {input_language} to {output_language}.\"),('human',\"{text_to_translate}\")])\n",
    "# model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "for part in chatbot.stream({\"input_language\":\"English\", \"output_language\":\"French\", \"text_to_translate\":\"I love programming.\"}):\n",
    "    print(part)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d76801",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "### Task 4 <a class=\"anchor\" id=\"task4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ddf302",
   "metadata": {},
   "source": [
    "Now you should have the tools to make a chatbot assistant that helps you with questions you may have using LCEL! Right?\n",
    "\n",
    "Prove it below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e4c9de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5943d4ee",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "In this notebook, you have gained the necessary basic skills to build LLM applications. \n",
    "\n",
    "These are essentially chains consisting of the LLM, which makes the predictions, the prompt instructions and how to guide the output to a certain form. \n",
    "\n",
    "In the next document, you'll learn how to provide context to your model in the form of data and what utility this can provide."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
